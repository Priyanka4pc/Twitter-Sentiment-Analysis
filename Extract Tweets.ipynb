{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the required libraries\n",
    "import requests\n",
    "from requests_oauthlib import OAuth1\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "from pandas import DataFrame\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get your consumer_key and consumer_secret from your twitter developer account\n",
    "consumer_key = \"XXXXXXXXXXXXXXXXXXXX\" \n",
    "consumer_secret = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type your keywords, you want to use to extract the tweets\n",
    "q = '#USElections2020 OR #USElections OR #2020Elections OR #2020Election OR #USPolitics OR #Elections2020 OR #US OR \\\n",
    "#Elections OR elections OR trump OR moscowmitch OR democrats OR #maga OR donaldtrump OR #voteblue OR bluewave OR \\\n",
    "biden OR bernie OR petebuttigieg OR republican OR liberal OR #berniesanders OR kag OR democrat OR #usa OR andrewyang \\\n",
    "OR #impotus  OR mikepence OR #trumpsucks OR #yanggang OR qanon OR gop OR pete -filter:retweets AND -filter:replies'\n",
    "\n",
    "# url to extract tweets\n",
    "url = 'https://api.Twitter.com/1.1/search/tweets.json'\n",
    "\n",
    "# set your parameters\n",
    "# maximum of 100 tweets can be extracted in one call\n",
    "pms = {'q':q,'count':100, 'lang':'en', 'tweet_mode':'extended'}\n",
    "\n",
    "# for authentication\n",
    "auth = OAuth1(consumer_key,consumer_secret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate your dataframe to store the tweets\n",
    "tweets_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection Status: OK, page =   1\n",
      "Connection Status: OK, page =   2\n",
      "Connection Status: OK, page =   3\n",
      "Connection Status: OK, page =   4\n",
      "Connection Status: OK, page =   5\n",
      "Connection Status: OK, page =   6\n",
      "Connection Status: OK, page =   7\n",
      "Connection Status: OK, page =   8\n",
      "Connection Status: OK, page =   9\n",
      "Connection Status: OK, page =   10\n",
      "Connection Status: OK, page =   11\n",
      '.....................................',
      "Connection Status: OK, page =   2000\n"
     ]
    }
    ],
   "source": [
    "# initiate the page_counter\n",
    "pages_counter = 0\n",
    "\n",
    "# total number of tweets to be extracted, 100*number_of_pages\n",
    "number_of_pages = 2000\n",
    "\n",
    "#start your loop for 2000 calls\n",
    "while pages_counter<number_of_pages:\n",
    "    pages_counter += 1\n",
    "    res = requests.get(url,params = pms,auth = auth)\n",
    "    \n",
    "    # print the connection status and call number\n",
    "    print(\"Connection Status: %s, page =  \" % res.reason,pages_counter )\n",
    "    \n",
    "    # get a json file of the extracted tweets\n",
    "    tweets = res.json()\n",
    "    # convert to dataframe\n",
    "    df = json_normalize(tweets['statuses'])\n",
    "    \n",
    "    # set the pms id so that same tweets are not extracted in every call\n",
    "    ids = [i['id'] for i in tweets['statuses']]\n",
    "    pms['max_id'] = min(ids)-1\n",
    "    \n",
    "    # add the extracted tweets to our initial dataframe\n",
    "    tweets_df = pd.concat([tweets_df, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215231, 187)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of tweets and columns\n",
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the useful columns\n",
    "tweets_df = tweets_df.filter(items = ['created_at','id','full_text','source','user.id','user.name','user.location','user.followers_count','user.lang'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215231, 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# incase of any duplicate tweets, drop the duplicate tweets \n",
    "tweets_df = tweets_df.drop_duplicates(subset = ['created_at','id'])\n",
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store \n",
    "tweets_df.to_csv('us_elections2020.csv',index=False, header = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
